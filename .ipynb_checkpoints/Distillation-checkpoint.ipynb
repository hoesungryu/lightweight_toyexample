{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Distillation \n",
    "\n",
    "\n",
    "<img src='./imgs/distillation.jpg'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive Mount \n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Value \n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BEST_ACC = 0 \n",
    "START_EPOCH = 0 # start from epoch 0 or last checkpoint epoch\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "print('==> Preparing data..')\n",
    "\n",
    "mean_nums = [0.485, 0.456, 0.406]\n",
    "std_nums = [0.229, 0.224, 0.225]\n",
    "transform_train = transforms.Compose([transforms.RandomResizedCrop(size=256),\n",
    "                                      transforms.RandomRotation(degrees=15),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean_nums, std_nums),\n",
    "])\n",
    "\n",
    "\n",
    "transform_test = transforms.Compose([transforms.Resize(256),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize(mean_nums, std_nums),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataSet & DataLoader\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True,)\n",
    "testloader = torch.utils.data.DataLoader(testset, \n",
    "                                         batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data size \n",
    "images, labels = next(iter(trainloader))\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teacher \n",
    "teacher = models.__dict__['resnet34'](pretrained=True)\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "num_classes =10\n",
    "in_features = teacher.fc.in_features\n",
    "teacher.fc = nn.Linear(in_features,num_classes)\n",
    "\n",
    "for name, child in teacher.named_children():\n",
    "    if name in ['layer3', 'layer4','fc']:\n",
    "        print(name + 'has been unfrozen.')\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "teacher = teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(teacher,(3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(teacher.parameters(), lr=1e-3,\n",
    "                      momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training\n",
    "def train(epoch,net):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx %1000 ==0:\n",
    "#             print('Loss: %.3f | Acc: %.3f%% (%d/%d)' %(train_loss/(batch_idx+1),100.*correct/total, correct, total))\n",
    "            print('Loss: %.3f | Acc: %.3f%% ' %(train_loss/(batch_idx+1),100.*correct/total))\n",
    "    \n",
    "def train_distillation(epoch, student, teacher):\n",
    "    print('\\nEpoch: %d' % epoch)\n",
    "    student.train()\n",
    "    teacher.eval()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = student(inputs) # student\n",
    "        teacher_outputs = teacher(inputs) # teacher \n",
    "\n",
    "        batch_size = targets.shape[0]\n",
    "        loss = loss_fn_kd(outputs, targets, teacher_outputs) / batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        if batch_idx %1000 ==0:\n",
    "            print('Loss: %.3f | Acc: %.3f%% ' %(train_loss/(batch_idx+1),100.*correct/total))\n",
    "     \n",
    "    \n",
    "def test(epoch,net):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN \n",
    "for epoch in range(start_epoch, ):\n",
    "    train(epoch,teacher)\n",
    "    test(epoch,teacher)\n",
    "    exp_lr_scheduler.step()\n",
    "    torch.save(teacher.state_dict(), './gdrive/MyDrive/Colab Notebooks/data/'+f'teacher_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student( User-defined Network; two CNN layer )\n",
    "class Student(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Student, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(59536, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "        x =  self.relu(self.fc1(x))\n",
    "        x =  self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "student = Student()\n",
    "student = student.to(device)\n",
    "summary(student,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(student.parameters(), lr=1e-3,\n",
    "                      momentum=0.9)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn_kd(outputs, labels, teacher_outputs, T=1, alpha=0.3):\n",
    "  # outputs = model에 넣었을때 나온것 \n",
    "  # labels =true label \n",
    "  # teacher_outputs = teacher_model(data) # teacher model eval()\n",
    "  # params = T & alpha \n",
    "    \"\"\"\n",
    "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
    "    \"Hyperparameters\": temperature and alpha\n",
    "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
    "    and student expects the input tensor to be log probabilities\n",
    "    \"\"\"\n",
    "    # alpha = params.alpha\n",
    "    # T = params.temperature\n",
    "    KD_loss = nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
    "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
    "              F.cross_entropy(outputs, labels) * (1. - alpha)\n",
    "\n",
    "\n",
    "\n",
    "    return KD_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Teacher dict \n",
    "\n",
    "PATH = './gdrive/MyDrive/Colab Notebooks/data/'+f'teacher_{epoch}.pth'\n",
    "# num_classes =10\n",
    "\n",
    "# teacher = models.__dict__['resnet34']()\n",
    "# in_features = teacher.fc.in_features\n",
    "# teacher.fc = nn.Linear(in_features,num_classes)\n",
    "\n",
    "teacher.load_state_dict(torch.load(PATH))\n",
    "for param in teacher.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "teacher.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('start KD ... ')\n",
    "for epoch in range(start_epoch, start_epoch+10):\n",
    "    train_distillation(student, teacher)\n",
    "    test(epoch,student)\n",
    "    exp_lr_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
